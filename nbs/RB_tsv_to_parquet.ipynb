{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0777bb5-a0f1-488d-b49e-4ca9a92a650e",
   "metadata": {},
   "source": [
    "# process refine bio meta data to parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvn9qux36a",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è This Notebook is Not Runnable\n",
    "\n",
    "**This notebook documents the conversion process used in the original development environment.** The original 3GB TSV file (`metadata_HOMO_SAPIENS.tsv`) is **not** included in this repository due to its size.\n",
    "\n",
    "**What's available:**\n",
    "- ‚úÖ The **output** Parquet file (`data/metadata_HOMO_SAPIENS.parquet`, 71MB) is tracked via Git LFS\n",
    "- ‚úÖ This notebook serves as documentation of how the conversion was performed\n",
    "- ‚ùå The source TSV is not here and cannot be re-run\n",
    "\n",
    "If you need to work with the metadata, use the Parquet file directly (see other notebooks like `detect_potential_clin.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5af98",
   "metadata": {},
   "source": [
    "## Context: Why Refine.bio Matters\n",
    "\n",
    "**The Problem:** NCBI GEO hosts ~1 million human RNA samples with mostly unstructured metadata and no efficient search engine. Each experiment uses different formats and processing pipelines.\n",
    "\n",
    "**The Solution:** [Refine.bio](https://www.refine.bio/) uniformly processed ~400,000 RNA samples (mostly microarray) with:\n",
    "- **Normalized expression data**: SCAN + quantile normalization applied uniformly\n",
    "- **Structured metadata**: Universal `refinebio_*` columns + union of all original GEO columns as a queryable table\n",
    "- **Quality control**: Pre-filtered for expression coverage\n",
    "\n",
    "**The Challenge:** Refine.bio's metadata is a 3GB TSV with 6,700+ columns and quirky values. This notebook converts it to an efficient 71MB Parquet file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86868a44",
   "metadata": {},
   "source": "## Why Convert to Parquet?\n\nWhile I have nothing but respect for the work behind Refine.bio and seriously value what they did, the data formats they used are hard to work with.\n\nThe original metadata file is a **3GB TSV** with problematic values (like barcode overflows). Below, I use Polars' lazy evaluation to:\n1. **Stream the TSV** without loading into memory (using `scan_csv`)\n2. **Infer a robust schema** by scanning all 420K rows during the stream\n3. **Fix malformed values** discovered during inference (via `null_values` parameter)\n4. **Write to Parquet** in a single pass\n\n**Result:** A **71MB Parquet file** that's ~40√ó smaller and can be lazily queried without loading into RAM.\n\n**Why this matters for paired data discovery:** Parquet's columnar format makes it efficient to scan 6,700 metadata columns looking for clinical response indicators, without loading the full dataset into memory.\n\n## Why Scan All 420K Rows for Schema Inference?\n\nPolars defaults to inferring types from the **first 100 rows**. This irregular TSV had multiple edge cases:\n\n**Problem 1: Nulls followed by values**\n- Some columns had 1,000+ null rows, then `3.14` appeared ‚Üí should be `Float64`\n- With shallow inference: Polars sees only nulls ‚Üí types as `String` ‚Üí crashes when it hits the float\n\n**Problem 2: Integers that become floats**\n- Column looks like `[1, 2, 3, ...]` for 5,000 rows ‚Üí Polars infers `Int64`\n- Then `3.14` appears at row 10,000 ‚Üí type mismatch error\n\n**Problem 3: Integer overflow**\n- The barcode column had values like `5503934202250110435328` (too large for `Int64`)\n- Even with full scan, Polars saw numbers first ‚Üí inferred `Int64` ‚Üí **overflow error**\n- **Solution:** Added this value to `null_values` parameter to treat overflow as null\n\n**Key insight:** By scanning all 420K rows (`infer_schema_length=420000`), I caught these issues during inference rather than mid-conversion. Understanding Polars' inference behavior allowed me to handle this highly irregular file without errors‚Äîand without loading 60GB into memory."
  },
  {
   "cell_type": "markdown",
   "id": "eqo0srj8h0d",
   "metadata": {},
   "source": "## What is Polars and Why Use It?\n\n**Polars** is a fast dataframe library (alternative to Pandas) designed for:\n- **Lazy evaluation**: Build a query plan without executing it (`pl.scan_csv()` instead of `pl.read_csv()`)\n- **Memory efficiency**: Stream operations without loading the entire dataset into RAM\n- **Speed**: Written in Rust, optimized for columnar data processing\n\n### Key Difference: `read` vs `scan`\n\n```python\n# pl.read_csv() - EAGER execution\ndf = pl.read_csv(\"huge_file.csv\")  \n# üí• Immediately loads entire file into memory (60GB RAM used)\n# ‚úÖ Returns DataFrame you can inspect/manipulate immediately\n# ‚úÖ Can access .schema, .head(), etc.\n\n# pl.scan_csv() - LAZY execution  \nlf = pl.scan_csv(\"huge_file.csv\")\n# ‚úÖ Only builds a query plan (~0 MB RAM)\n# ‚úÖ Can infer schema during streaming (see infer_schema_length)\n# ‚ùå Cannot access data until you execute (.collect(), .sink_parquet(), etc.)\nlf.select([\"col1\"]).sink_parquet(\"output.parquet\")  # Streams without full load\n```\n\n### Schema Inference in Lazy Mode\n\n**Important:** `scan_csv` CAN infer schemas, but it happens during execution (streaming), not upfront:\n\n```python\n# This WORKS - schema inference happens while streaming to Parquet\npl.scan_csv(\n    \"file.tsv\",\n    infer_schema_length=420000,  # Scan 420K rows to infer types\n    null_values=[\"NA\", \"overflow_value\"]\n).sink_parquet(\"output.parquet\")\n# ‚úÖ Polars reads in batches, infers types, streams to Parquet\n# ‚úÖ Never loads full dataset into memory\n```\n\n**Key insight:** You don't need to `read_csv` first to get a schema. The `scan_csv` ‚Üí `sink_parquet` pipeline infers types on-the-fly during streaming.\n\n## What is a Schema?\n\nA **schema** defines the structure of your data:\n- **Column names**: `[\"sample_id\", \"age\", \"tissue\", ...]`\n- **Data types**: `String`, `Int64`, `Float64`, `Boolean`, etc.\n\nExample:\n```python\n{\"sample_id\": String, \"age\": Int64, \"tumor_size\": Float64}\n```\n\n**Why schemas matter for TSV files:**\n- Polars must **infer** types by scanning rows (default: first 100 rows)\n- If unusual values appear later, inference fails or mis-types columns\n- This TSV had multiple irregularities that required full-length inference to catch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f7c11-1055-42a3-961a-687a9865f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, io, os, sys\n",
    "import typing as tp\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pacsv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e5b01-4340-4aa2-bff3-2195e06ce8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3c212-71b5-4ef2-8e68-f48616be86b7",
   "metadata": {},
   "source": [
    "## helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225b064-66b4-44e3-91a1-c2ff276f1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_schema_to_json(df: pl.DataFrame, out: Path) -> None:\n",
    "    \"\"\"\n",
    "    Serialize a Polars DataFrame schema to JSON as {col: dtype_name}.\n",
    "    dtype_name ‚àà {\"Utf8\",\"String\",\"Int64\",\"Float64\",\"Boolean\",...}\n",
    "    \"\"\"\n",
    "    schema = {name: str(dtype) for name, dtype in df.schema.items()}\n",
    "    out.write_text(json.dumps(schema, indent=2))\n",
    "def json_schema_to_polars_dtypes(json_path: Path) -> dict[str, pl.DataType]:\n",
    "    \"\"\"\n",
    "    Read {col: dtype_name} and map to Polars dtypes.\n",
    "    Unknown names default to Utf8 (conservative).\n",
    "    \"\"\"\n",
    "    name_to_pl = {\n",
    "        \"Utf8\": pl.Utf8, \"String\": pl.Utf8,\n",
    "        \"Int64\": pl.Int64, \"Float64\": pl.Float64, \"Boolean\": pl.Boolean,\n",
    "        \"Int32\": pl.Int32, \"Float32\": pl.Float32, \"Date\": pl.Date,\n",
    "        \"Datetime\": pl.Datetime, \"Time\": pl.Time, \"Categorical\": pl.Categorical,\n",
    "    }\n",
    "    schema = json.loads(json_path.read_text())\n",
    "    return pl.Schema({c: name_to_pl.get(t, pl.Utf8) for c, t in schema.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbda70b-af30-4bc3-b8bc-71e403a5262c",
   "metadata": {},
   "source": [
    "## paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93194613-740b-4eb8-a295-c05ac272f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original path from development environment (not in this repo)\n",
    "# root = Path(\"/mnt/hdd/jesse_archive/stampformer_archive/refine_bio/HOMO_SAPIENS\")\n",
    "\n",
    "# Paths for this repository\n",
    "from pathlib import Path\n",
    "root = Path(\"../data\")  # Relative to nbs/ directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0029bf5-eb66-414e-ae42-e2881a19ec7e",
   "metadata": {},
   "source": "## Stream TSV to Parquet\n\nThe approach below uses Polars' lazy evaluation to convert the 3GB TSV to Parquet **without loading it into memory**. \n\n**How it works:**\n1. `scan_csv` creates a lazy query plan (no data loaded yet)\n2. `infer_schema_length=420000` tells Polars to scan all rows during execution to infer types\n3. `sink_parquet` triggers execution‚ÄîPolars reads in batches, infers schema, writes to Parquet\n4. Memory usage: Only one batch in RAM at a time (vs. 60GB for full load)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac655164-40dd-45d2-89cc-82ba4a76bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated paths for this repository\n",
    "data_dir = Path('../data')  # Relative to nbs/ directory\n",
    "\n",
    "# Source TSV (NOT in this repo - original was 3GB)\n",
    "tsv_meta_path = data_dir / 'metadata_HOMO_SAPIENS.tsv'  # ‚ö†Ô∏è Does not exist here\n",
    "\n",
    "# Output files (schema and Parquet ARE in the repo)\n",
    "schema_path = data_dir / \"metadata_HOMO_SAPIENS_schema.json\"\n",
    "parquet_path = data_dir / \"metadata_HOMO_SAPIENS.parquet\"  # ‚úÖ Available via Git LFS\n",
    "\n",
    "# Downstream outputs\n",
    "potential_clin_path = data_dir / 'potential_clin_data.csv'\n",
    "\n",
    "sep = '\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ab7ee-0b21-44f6-b09c-776e2bf44a71",
   "metadata": {},
   "outputs": [],
   "source": "lazy_data = pl.scan_csv(\n    str(tsv_meta_path),\n    separator=sep,\n    has_header=True,\n    infer_schema_length=420000,\n    null_values=[\"\", \"NA\", \"NaN\", \"null\", \"None\",'5503934202250110435328'], \n    ignore_errors=False,\n    low_memory=True,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e605dc3-d6c0-4c59-83c1-325b87a756a9",
   "metadata": {},
   "outputs": [],
   "source": "lazy_data.sink_parquet(\n    parquet_path,\n    compression=\"zstd\",\n    compression_level=4,\n    row_group_size=64_000,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "gn9xigbm6dm",
   "metadata": {},
   "source": "## ‚úÖ Key Takeaways: Efficient Large File Processing\n\nThis conversion demonstrates several important Polars concepts:\n\n1. **Lazy evaluation wins**: `scan_csv` ‚Üí `sink_parquet` never loads the full dataset into memory\n   - Memory usage: ~1-2GB (batch size) vs. ~60GB (full eager load)\n   \n2. **Schema inference during streaming**: Setting `infer_schema_length=420000` tells Polars to scan all rows during the stream to correctly infer types\n   - Catches edge cases like nulls-then-floats, ints-that-become-floats, overflow values\n   \n3. **Problem-solving with null_values**: When I hit the barcode overflow error, I added `'5503934202250110435328'` to `null_values` to treat it as missing data rather than crashing\n   - This is faster than pre-cleaning the TSV or manually specifying a schema\n\n4. **Parquet compression**: The result is 40√ó smaller (71MB vs 3GB) and faster to query\n   - `zstd` compression with level 4 balances compression ratio and speed\n   - `row_group_size=64_000` optimizes for scanning metadata columns\n\n**Bottom line:** Understanding Polars' lazy evaluation and schema inference allowed me to handle a highly irregular 3GB file without errors or excessive memory usage."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
